{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Aperture Aperture is a Particle-in-Cell (PIC) code framework that targets both traditional CPUs and modern GPUs. It is designed to be fast, flexible, and easy to extend.","title":"Home"},{"location":"#aperture","text":"Aperture is a Particle-in-Cell (PIC) code framework that targets both traditional CPUs and modern GPUs. It is designed to be fast, flexible, and easy to extend.","title":"Aperture"},{"location":"0-intro/","text":"Introduction The name Aperture is a recursive acronym that stands for: \" A perture is a code for P articles, E lectrodynamics, and R adiative T ransfer at U ltra- R elativistic E nergies\". As the name suggests, its main goal is to simulation the interaction of ultra-relativistic particles with electromagnetic field and radiation, which occurs very often in astrophysical scenarios. Downloading Aperture To download and compile Aperture , you can clone it from the github repo: $ git clone https://github.com/fizban007/Aperture4.git Development in a Container The easiest way to compile and develop Aperture is to use a Docker container. The official development container is fizban007/aperture_dev . The image is generated using the develop.Dockerfile under the main Aperture git repo. It contains all the necessary libraries to develop Aperture . In order to avoid using the root user in the container, a default user called developer is created by default. It is recommended to mount a Docker volume (or a host directory) to the root directory /code which is writable by user developer . One can do this using: $ docker pull fizban007/aperture_dev $ docker volume create vol1 $ docker run -it --name Aperture_dev --mount source=vol1,target=/code fizban007/aperture_dev This will create a container named Aperture_dev which mounts the volume at the correct location. Once in the container, one can clone the main code repo, use VS Code to attach to the container and carry out development and debugging there. Development on a Host Machine To compile Aperture from scratch, you will need to have several libraries ready: A modern C++ compiler that supports c++17 , e.g.: gcc>=7.0 or clang>=3.9 or intel>=19.0 An MPI implementation, e.g. openmpi , intel-mpi , mpich , etc. An hdf5 library, preferably an mpi -enabled version The C++ library boost>=1.54 (Optional) The Cuda toolkit cuda>=11 that supports at least c++17 (Optional) The ROCm toolkit provided by AMD To help with this, a bunch of configuration scripts under the machines directory should contain the necessary module load for the given machine to compile the code. Compiling the Code Whether you use a Docker container or manage the libraries on your own, you can compile the code using the following: $ mkdir build & cd build $ cmake .. $ make -j This will compile the main Aperture library, the test cases, and the problems. The binary files for specific setups will be generated inside the corresponding problems/XXX/bin directory. The following are the options (and their default values) to cmake which controls some compile-time configurations: -Duse_cuda=0 : If this option is 1, include the GPU part of Aperture and compile it as CUDA code. -Duse_hip=0 : If this option is 1, include the GPU part of Aperture and compile it as the AMD HIP code. Note that one can specify both of these two options. In that case, it will try to compile the code through the HIP but using its CUDA implementation. -Dbuild_tests=1 : If this option is 1, build the unit test suite. -Duse_double=0 : If this option is 1, use double precision for all floating point calculations. -Dlib_only=0 : If this option is 1, only build the Aperture library, but not any simulation setups in the problems directory. -Duse_libcpp=0 : This option is specifically for the situation where you want to compile the code with clang and link to libc++ instead of libstdc++ . Generally keep it off unless you know what you are doing. Even then, it is not guaranteed to work. Apart from these options, standard CMake options apply as well. For example, the code will compile in Release mode by default. To change it to Debug , append -DCMAKE_BUILD_TYPE=Debug to the cmake command. To run a small suite of unit tests, run make check in the build directory. Running Simulations To run a quick test simulation, checkout the training directory in problems . It contains several self-contained simple setups that can run without fiddling with configuration files. For a more in-depth guide on simulation setup, checkout Setting up a Simulation , or take a look at one of the tutorials.","title":"Intro"},{"location":"0-intro/#introduction","text":"The name Aperture is a recursive acronym that stands for: \" A perture is a code for P articles, E lectrodynamics, and R adiative T ransfer at U ltra- R elativistic E nergies\". As the name suggests, its main goal is to simulation the interaction of ultra-relativistic particles with electromagnetic field and radiation, which occurs very often in astrophysical scenarios.","title":"Introduction"},{"location":"0-intro/#downloading-aperture","text":"To download and compile Aperture , you can clone it from the github repo: $ git clone https://github.com/fizban007/Aperture4.git","title":"Downloading Aperture"},{"location":"0-intro/#development-in-a-container","text":"The easiest way to compile and develop Aperture is to use a Docker container. The official development container is fizban007/aperture_dev . The image is generated using the develop.Dockerfile under the main Aperture git repo. It contains all the necessary libraries to develop Aperture . In order to avoid using the root user in the container, a default user called developer is created by default. It is recommended to mount a Docker volume (or a host directory) to the root directory /code which is writable by user developer . One can do this using: $ docker pull fizban007/aperture_dev $ docker volume create vol1 $ docker run -it --name Aperture_dev --mount source=vol1,target=/code fizban007/aperture_dev This will create a container named Aperture_dev which mounts the volume at the correct location. Once in the container, one can clone the main code repo, use VS Code to attach to the container and carry out development and debugging there.","title":"Development in a Container"},{"location":"0-intro/#development-on-a-host-machine","text":"To compile Aperture from scratch, you will need to have several libraries ready: A modern C++ compiler that supports c++17 , e.g.: gcc>=7.0 or clang>=3.9 or intel>=19.0 An MPI implementation, e.g. openmpi , intel-mpi , mpich , etc. An hdf5 library, preferably an mpi -enabled version The C++ library boost>=1.54 (Optional) The Cuda toolkit cuda>=11 that supports at least c++17 (Optional) The ROCm toolkit provided by AMD To help with this, a bunch of configuration scripts under the machines directory should contain the necessary module load for the given machine to compile the code.","title":"Development on a Host Machine"},{"location":"0-intro/#compiling-the-code","text":"Whether you use a Docker container or manage the libraries on your own, you can compile the code using the following: $ mkdir build & cd build $ cmake .. $ make -j This will compile the main Aperture library, the test cases, and the problems. The binary files for specific setups will be generated inside the corresponding problems/XXX/bin directory. The following are the options (and their default values) to cmake which controls some compile-time configurations: -Duse_cuda=0 : If this option is 1, include the GPU part of Aperture and compile it as CUDA code. -Duse_hip=0 : If this option is 1, include the GPU part of Aperture and compile it as the AMD HIP code. Note that one can specify both of these two options. In that case, it will try to compile the code through the HIP but using its CUDA implementation. -Dbuild_tests=1 : If this option is 1, build the unit test suite. -Duse_double=0 : If this option is 1, use double precision for all floating point calculations. -Dlib_only=0 : If this option is 1, only build the Aperture library, but not any simulation setups in the problems directory. -Duse_libcpp=0 : This option is specifically for the situation where you want to compile the code with clang and link to libc++ instead of libstdc++ . Generally keep it off unless you know what you are doing. Even then, it is not guaranteed to work. Apart from these options, standard CMake options apply as well. For example, the code will compile in Release mode by default. To change it to Debug , append -DCMAKE_BUILD_TYPE=Debug to the cmake command. To run a small suite of unit tests, run make check in the build directory.","title":"Compiling the Code"},{"location":"0-intro/#running-simulations","text":"To run a quick test simulation, checkout the training directory in problems . It contains several self-contained simple setups that can run without fiddling with configuration files. For a more in-depth guide on simulation setup, checkout Setting up a Simulation , or take a look at one of the tutorials.","title":"Running Simulations"},{"location":"1-setup/","text":"Setting up a Simulation The following code is the boiler plate for setting up a PIC simulation in Aperture : #include \"framework/config.h\" #include \"framework/environment.h\" #include \"systems/data_exporter.h\" #include \"systems/domain_comm.h\" #include \"systems/field_solver_cartesian.h\" #include \"systems/ptc_updater.h\" using namespace Aperture; int main(int argc, char* argv[]) { typedef Config<2> Conf; // Specify that this is a 2D simulation // Initialize the simulation environment auto &env = sim_environment::instance(&argc, &argv); // Choose execution policy depending on compile options (GPU or CPU) using exec_policy = exec_policy_dynamic<Conf>; // Setting up domain decomposition domain_comm<Conf, exec_policy_dynamic> comm; // Setting up the simulation grid grid_t<Conf> grid(comm); // Add a particle pusher auto pusher = env.register_system< ptc_updater<Conf, exec_policy_dynamic, coord_policy_cartesian>>(grid, &comm); // Add a field solver auto solver = env.register_system< field_solver<Conf, exec_policy_dynamic, coord_policy_cartesian>>(grid, &comm); // Setup data output auto exporter = env.register_system<data_exporter<Conf, exec_policy_dynamic>>(grid, &comm); // Call the init() method of all systems env.init(); // Prepare initial conditions ... // Enter the main simulation loop env.run(); } The Config class contains compile-time configurations for the code, including the dimensionality (2 in the above example), data type for floating point numbers, particle pusher type, and indexing scheme. The method register_system<T> constructs a :doc: /api/framework/system , puts it in the registry, and returns a pointer to it. When init() or run() is called, all the init() and run() methods of the registered systems are run in the order they are registered. In the above example, at every timestep, the code will first call the ptc_updater , then the field_solver , then the data_exporter . There are two main ways to customize the problem setup, namely through the config file config.toml , or programmatically through coding initial or boundary conditions. Config File Every system has a number of parameters that can be customized through run time parameters. All parameters are read from a configuration file in the toml <https://github.com/toml-lang/toml> _ format. By default, the code will look for a file named config.toml in the same directory as the executable. A different config file can also be specified through a launch parameter: $ ./aperture -c some/other/config/file.toml Parameters are stored in an instance of :ref: params_store in the :ref: sim_environment class. One can also define all the required parameters programmatically: sim_env().params().add(\"dt\", 0.01); sim_env().params().add(\"max_ptc_num\", 100); Since systems may use parameters in their constructors, one should add whatever needed parameters before initializing any systems. Source Code Some things need to be specified in the source code and require a recompile, e.g. non-trivial initial conditions. For example, one can assign an initial function to some field: vector_field<Conf> *B0; // Declare a pointer to the background B env.get_data(\"B0\", &B0); // Point it to the \"B0\" data component in the registry double Bp = 100.0; // Set a characteristic value for B B0->set_value(0, [Bp](auto r, auto theta, auto phi) { return Bp / square(r); }); // Set the 0th component (B_r) to a monopole field in spherical coordinates Nontrivial boundary conditions can be more difficult to set up, especially time-dependent ones which requires the user to write a customized system . Please refer to The Aperture Framework for an explanation of how to write a custom system .","title":"Quick Setup"},{"location":"1-setup/#setting-up-a-simulation","text":"The following code is the boiler plate for setting up a PIC simulation in Aperture : #include \"framework/config.h\" #include \"framework/environment.h\" #include \"systems/data_exporter.h\" #include \"systems/domain_comm.h\" #include \"systems/field_solver_cartesian.h\" #include \"systems/ptc_updater.h\" using namespace Aperture; int main(int argc, char* argv[]) { typedef Config<2> Conf; // Specify that this is a 2D simulation // Initialize the simulation environment auto &env = sim_environment::instance(&argc, &argv); // Choose execution policy depending on compile options (GPU or CPU) using exec_policy = exec_policy_dynamic<Conf>; // Setting up domain decomposition domain_comm<Conf, exec_policy_dynamic> comm; // Setting up the simulation grid grid_t<Conf> grid(comm); // Add a particle pusher auto pusher = env.register_system< ptc_updater<Conf, exec_policy_dynamic, coord_policy_cartesian>>(grid, &comm); // Add a field solver auto solver = env.register_system< field_solver<Conf, exec_policy_dynamic, coord_policy_cartesian>>(grid, &comm); // Setup data output auto exporter = env.register_system<data_exporter<Conf, exec_policy_dynamic>>(grid, &comm); // Call the init() method of all systems env.init(); // Prepare initial conditions ... // Enter the main simulation loop env.run(); } The Config class contains compile-time configurations for the code, including the dimensionality (2 in the above example), data type for floating point numbers, particle pusher type, and indexing scheme. The method register_system<T> constructs a :doc: /api/framework/system , puts it in the registry, and returns a pointer to it. When init() or run() is called, all the init() and run() methods of the registered systems are run in the order they are registered. In the above example, at every timestep, the code will first call the ptc_updater , then the field_solver , then the data_exporter . There are two main ways to customize the problem setup, namely through the config file config.toml , or programmatically through coding initial or boundary conditions.","title":"Setting up a Simulation"},{"location":"1-setup/#config-file","text":"Every system has a number of parameters that can be customized through run time parameters. All parameters are read from a configuration file in the toml <https://github.com/toml-lang/toml> _ format. By default, the code will look for a file named config.toml in the same directory as the executable. A different config file can also be specified through a launch parameter: $ ./aperture -c some/other/config/file.toml Parameters are stored in an instance of :ref: params_store in the :ref: sim_environment class. One can also define all the required parameters programmatically: sim_env().params().add(\"dt\", 0.01); sim_env().params().add(\"max_ptc_num\", 100); Since systems may use parameters in their constructors, one should add whatever needed parameters before initializing any systems.","title":"Config File"},{"location":"1-setup/#source-code","text":"Some things need to be specified in the source code and require a recompile, e.g. non-trivial initial conditions. For example, one can assign an initial function to some field: vector_field<Conf> *B0; // Declare a pointer to the background B env.get_data(\"B0\", &B0); // Point it to the \"B0\" data component in the registry double Bp = 100.0; // Set a characteristic value for B B0->set_value(0, [Bp](auto r, auto theta, auto phi) { return Bp / square(r); }); // Set the 0th component (B_r) to a monopole field in spherical coordinates Nontrivial boundary conditions can be more difficult to set up, especially time-dependent ones which requires the user to write a customized system . Please refer to The Aperture Framework for an explanation of how to write a custom system .","title":"Source Code"},{"location":"2-framework/","text":"The Aperture Framework The Aperture framework is inspired by the Entity-Component-System (ECS) paradigm of modern game engines. In a sense, a numerical simulation is very similar to a video game, where different quantities are evolved over time in a giant loop, inside which every module is called sequentially to do their job. A simulation can be much simpler than a video game, since usually no interactivity is needed. However, there can still be very complex logical dependency between different modules, and this framework is designed to make it relatively easy to add new modules or to setup new physical scenarios. A simulation code does not (usually) need to create and destroy \"entities\" in real time, like a game would. Therefore, in Aperture , there are only two main categories of classes: system and data . data is what holds the simulation data, e.g. fields and particles, while system refers to any module that works on the data, e.g. pushing particles or evolving fields. The benefit of such a configuration is that both system and data are flexible and can be plugged in and out depending on the problem. It is also very straightforward to handle data IO, since we can simply serialize a list of named data objects. For lack of a better name, the sim_environment class is a coordinator that ties things together. It keeps a registry of systems and data components, and calls every system in order in a giant loop for the duration of the simulation. Systems Every system derives from the common base class system_t . There are three virtual functions a system can override: register_data_components() , init() , and update() . register_data_components() A system needs to work on some data components. Depending on what systems are initialized, some data components may or may not be used. Therefore, systems are responsible for managing their own data dependency. For example, a field_solver system needs to work on \\(\\mathbf{E}\\) and \\(\\mathbf{B}\\) fields, so it needs to register this dependency by overriding the register_data_components() function: void register_data_components() { // E is a raw pointer to vector_field<Conf> E = m_env.register_data<vector_field<Conf>>( \"E\", m_grid, field_type::edge_centered); B = m_env.register_data<vector_field<Conf>>( \"B\", m_grid, field_type::edge_centered); } E and B are pointers to vector_field<Conf> and are constructed here. register_data_components() takes an std::string for a name, followed by parameters that are passed to the constructor of the data component. If these data components are registered already by another system under the same name, then the register_data() function will only return the pointer. No two components with the same name can co-exist. register_data_components() is called right after the constructor of the system, in register_system() . init() update() Data Components Every data component derives from the common base class data_t .","title":"Framework"},{"location":"2-framework/#the-aperture-framework","text":"The Aperture framework is inspired by the Entity-Component-System (ECS) paradigm of modern game engines. In a sense, a numerical simulation is very similar to a video game, where different quantities are evolved over time in a giant loop, inside which every module is called sequentially to do their job. A simulation can be much simpler than a video game, since usually no interactivity is needed. However, there can still be very complex logical dependency between different modules, and this framework is designed to make it relatively easy to add new modules or to setup new physical scenarios. A simulation code does not (usually) need to create and destroy \"entities\" in real time, like a game would. Therefore, in Aperture , there are only two main categories of classes: system and data . data is what holds the simulation data, e.g. fields and particles, while system refers to any module that works on the data, e.g. pushing particles or evolving fields. The benefit of such a configuration is that both system and data are flexible and can be plugged in and out depending on the problem. It is also very straightforward to handle data IO, since we can simply serialize a list of named data objects. For lack of a better name, the sim_environment class is a coordinator that ties things together. It keeps a registry of systems and data components, and calls every system in order in a giant loop for the duration of the simulation.","title":"The Aperture Framework"},{"location":"2-framework/#systems","text":"Every system derives from the common base class system_t . There are three virtual functions a system can override: register_data_components() , init() , and update() .","title":"Systems"},{"location":"2-framework/#register_data_components","text":"A system needs to work on some data components. Depending on what systems are initialized, some data components may or may not be used. Therefore, systems are responsible for managing their own data dependency. For example, a field_solver system needs to work on \\(\\mathbf{E}\\) and \\(\\mathbf{B}\\) fields, so it needs to register this dependency by overriding the register_data_components() function: void register_data_components() { // E is a raw pointer to vector_field<Conf> E = m_env.register_data<vector_field<Conf>>( \"E\", m_grid, field_type::edge_centered); B = m_env.register_data<vector_field<Conf>>( \"B\", m_grid, field_type::edge_centered); } E and B are pointers to vector_field<Conf> and are constructed here. register_data_components() takes an std::string for a name, followed by parameters that are passed to the constructor of the data component. If these data components are registered already by another system under the same name, then the register_data() function will only return the pointer. No two components with the same name can co-exist. register_data_components() is called right after the constructor of the system, in register_system() .","title":"register_data_components()"},{"location":"2-framework/#init","text":"","title":"init()"},{"location":"2-framework/#update","text":"","title":"update()"},{"location":"2-framework/#data-components","text":"Every data component derives from the common base class data_t .","title":"Data Components"},{"location":"3-units/","text":"Units Aperture uses a unit system based on the Heaviside-Lorentz units . In the HL units, Maxwell equations look like: \\[ \\begin{align} \\nabla\\cdot\\mathbf{E} &= \\rho \\\\ \\nabla\\cdot\\mathbf{B} &= 0 \\\\ \\frac{\\partial \\mathbf{E}}{\\partial t} &= c\\nabla\\times\\mathbf{B} - \\mathbf{J} \\\\ \\frac{\\partial \\mathbf{B}}{\\partial t} &= -c\\nabla\\times\\mathbf{E} \\end{align} \\] In addition, we need to choose a length scale and time scale to make the equations dimensionless. We demand that \\(c = 1\\) , therefore choosing a length scale automatically determines the appropriate time scale, and vice versa. Energy is measured using units of \\(m_ec^2\\) , while particle momentum is measured in units of \\(m_ec\\) . If we specify a unit of time as \\(t_0 = \\omega_0^{-1}\\) , the Lorentz force then takes the dimensionless form: \\[ \\frac{d\\mathbf{p}}{m_ec\\omega_0 dt} = \\frac{e}{m_ec\\omega_0}\\left(\\mathbf{E} + \\frac{\\mathbf{v}}{c}\\times\\mathbf{B}\\right), \\] It is therefore appropriate to use \\(eE/m_ec\\omega_0\\) as the dimensionless form of the electric (or magnetic) field. The dimensionless value of the magnetic field is therefore equal to the dimensionless ratio \\(\\omega_B/\\omega_0\\) , or the dimensionless electron cyclotron frequency. The dimensionless charge density can be defined using the Gauss's law \\(\\nabla\\cdot\\mathbf{E}=\\rho\\) : \\[ \\frac{eE}{m_ec\\omega_0}\\frac{c/\\omega_0}{x} = \\frac{e\\rho}{m_e\\omega_0^2}. \\] The right hand side is therefore the natural definition of dimensionless charge density. In the code units, the dimensionless plasma frequency is nicely related to the charge density as follows: \\[ \\omega_p \\equiv \\sqrt{\\frac{ne^2}{m_e}} = \\sqrt{\\frac{e\\rho}{m_e\\omega_0^2}}\\omega_0. \\] In other words, the dimensionless plasma frequency is simply the square root of the dimensionless charge density, \\(\\tilde{\\omega}_p = \\sqrt{\\tilde{\\rho}}\\) . Tilde here means the dimensionless quantity. This relationship is very convenient to use in plasma simulations. In HL units, the energy density of electric/magnetic field is \\(\\mathbf{E}^2/2\\) or \\(\\mathbf{B}^2/2\\) respectively, in contrast with the corresponding expressions of \\(\\mathbf{E}^2/8\\pi\\) and \\(\\mathbf{B}^2/8\\pi\\) in CGS units. The magnetization \\(\\sigma\\) is now defined as: \\[ \\sigma \\equiv \\frac{B^2}{nm_ec^2} = \\frac{(eB/m_ec\\omega_0)^2}{e^2nm_ec^2}m_e^2c^2\\omega_0^2 = \\frac{(eB/m_ec\\omega_0)^2}{e\\rho/m_e\\omega_0^2}. \\] Identifying the terms, one can see that \\(\\sigma = \\tilde{B}^2/\\tilde{\\rho}\\) . In an actual simulation, one needs to specify either a length scale \\(x_0\\) or a time scale \\(\\omega_0^{-1}\\) , and it will completely specify the dimensionless unit system. For example, for a global pulsar simulation, the natural length scale is either the stellar radius \\(R_*\\) , or the light cylinder radius \\(R_\\mathrm{LC}\\) . For a black hole simulation the natural length scale may be the gravitational radius \\(r_g\\) . For a reconnection simulation it is often okay to choose the upstream \\(\\omega_0 = \\omega_p\\) . In that case, one simply needs to initialize the upstream plasma density such that \\(\\tilde{\\rho} = 1\\) .","title":"Units"},{"location":"3-units/#units","text":"Aperture uses a unit system based on the Heaviside-Lorentz units . In the HL units, Maxwell equations look like: \\[ \\begin{align} \\nabla\\cdot\\mathbf{E} &= \\rho \\\\ \\nabla\\cdot\\mathbf{B} &= 0 \\\\ \\frac{\\partial \\mathbf{E}}{\\partial t} &= c\\nabla\\times\\mathbf{B} - \\mathbf{J} \\\\ \\frac{\\partial \\mathbf{B}}{\\partial t} &= -c\\nabla\\times\\mathbf{E} \\end{align} \\] In addition, we need to choose a length scale and time scale to make the equations dimensionless. We demand that \\(c = 1\\) , therefore choosing a length scale automatically determines the appropriate time scale, and vice versa. Energy is measured using units of \\(m_ec^2\\) , while particle momentum is measured in units of \\(m_ec\\) . If we specify a unit of time as \\(t_0 = \\omega_0^{-1}\\) , the Lorentz force then takes the dimensionless form: \\[ \\frac{d\\mathbf{p}}{m_ec\\omega_0 dt} = \\frac{e}{m_ec\\omega_0}\\left(\\mathbf{E} + \\frac{\\mathbf{v}}{c}\\times\\mathbf{B}\\right), \\] It is therefore appropriate to use \\(eE/m_ec\\omega_0\\) as the dimensionless form of the electric (or magnetic) field. The dimensionless value of the magnetic field is therefore equal to the dimensionless ratio \\(\\omega_B/\\omega_0\\) , or the dimensionless electron cyclotron frequency. The dimensionless charge density can be defined using the Gauss's law \\(\\nabla\\cdot\\mathbf{E}=\\rho\\) : \\[ \\frac{eE}{m_ec\\omega_0}\\frac{c/\\omega_0}{x} = \\frac{e\\rho}{m_e\\omega_0^2}. \\] The right hand side is therefore the natural definition of dimensionless charge density. In the code units, the dimensionless plasma frequency is nicely related to the charge density as follows: \\[ \\omega_p \\equiv \\sqrt{\\frac{ne^2}{m_e}} = \\sqrt{\\frac{e\\rho}{m_e\\omega_0^2}}\\omega_0. \\] In other words, the dimensionless plasma frequency is simply the square root of the dimensionless charge density, \\(\\tilde{\\omega}_p = \\sqrt{\\tilde{\\rho}}\\) . Tilde here means the dimensionless quantity. This relationship is very convenient to use in plasma simulations. In HL units, the energy density of electric/magnetic field is \\(\\mathbf{E}^2/2\\) or \\(\\mathbf{B}^2/2\\) respectively, in contrast with the corresponding expressions of \\(\\mathbf{E}^2/8\\pi\\) and \\(\\mathbf{B}^2/8\\pi\\) in CGS units. The magnetization \\(\\sigma\\) is now defined as: \\[ \\sigma \\equiv \\frac{B^2}{nm_ec^2} = \\frac{(eB/m_ec\\omega_0)^2}{e^2nm_ec^2}m_e^2c^2\\omega_0^2 = \\frac{(eB/m_ec\\omega_0)^2}{e\\rho/m_e\\omega_0^2}. \\] Identifying the terms, one can see that \\(\\sigma = \\tilde{B}^2/\\tilde{\\rho}\\) . In an actual simulation, one needs to specify either a length scale \\(x_0\\) or a time scale \\(\\omega_0^{-1}\\) , and it will completely specify the dimensionless unit system. For example, for a global pulsar simulation, the natural length scale is either the stellar radius \\(R_*\\) , or the light cylinder radius \\(R_\\mathrm{LC}\\) . For a black hole simulation the natural length scale may be the gravitational radius \\(r_g\\) . For a reconnection simulation it is often okay to choose the upstream \\(\\omega_0 = \\omega_p\\) . In that case, one simply needs to initialize the upstream plasma density such that \\(\\tilde{\\rho} = 1\\) .","title":"Units"},{"location":"4-two-stream/","text":"Tutorial: Two-stream Instability In this tutorial, we will set up a Docker environment on your laptop for development of the code, and use it to run a basic 1D simulation of the electron two-stream instability. Install Docker Desktop You can download and install Docker Desktop here, regardless of your operating system: https://www.docker.com/products/docker-desktop/ Run \"Docker Desktop\". The first time it will try to setup your machine for Docker containers, so expect waiting for several minutes. But eventually you should see the main user interface of Docker Desktop. You can safely skip the initial tutorial. In order to prepare for our next steps, we want to create a persistent volume to hold all our development files. This can be done by clicking \"Volumes\" on the left panel, and then \"Create\" at the top right corner. Name it whatever you want, but I'll use vol_home as the volume name for the rest of this tutorial. Pull the development Docker image Click on the \"Search\" bar on the top right of Docker Desktop. Type in fizban007/aperture_dev . You will see my image shown below. You can download the image by clicking \"Pull\": Alternatively, you can open a terminal and run the following command docker pull fizban007/aperture_dev This will download the Docker image that contains all the development libraries necessary for compiling/debugging Aperture . The Docker image is pretty large at about 2.9GB, so the download will probably take a while depending on the speed of your internet connection. After the download is done, you should be able to see it under \"Images\" in the Docker Desktop. Now you can create a named container with this image and run it using the following command in a terminal (note: don't add extra spaces or the command may fail): $ docker run -it --mount source=vol_home,target=/home/developer --name Aperture_dev fizban007/aperture_dev Note that we referred to the volume we created in the previous step, and mounted it at /home/developer . The reason we mount the volume here is that the default user in the Docker image is developer , and this is the default home directory. The command above will create a new container with the name Aperture_dev and put you under a bash shell inside the Docker container. In the Docker Desktop GUI, you should be able to see our new container \"Aperture_dev\" in the \"Containers\" tab. You can checkout our PIC code Aperture using the following command inside the container: $ cd ~ $ git clone https://github.com/fizban007/Aperture4.git --branch develop This will create a directory called Aperture4 in your mounted volume (inside the home directory of user developer ). This will be the directory we work with in the future. Note we cloned the develop branch, which is the most up-to-date and actively maintained one. We only need to create the container for one time. Afterwards, if you want to restart the container, the docker run command above will fail because we already have a container named \"Aperture_dev\". Instead, you can do: $ docker start Aperture_dev $ docker attach This will start the container and put you under a command line prompt. Install Visual Studio Code Download and install VS Code from here: https://code.visualstudio.com/ Run VS Code. You will be directed to a \"Get started\" page. Click on the 5th icon on the left panel marked \"Extensions\". We want to install the \"Dev Containers\" extension to allow us to attach to a running Docker container. After installing \"Dev Containers\" (and possibly reloading VS Code), you should be able see a green button at the lower left corner of the window: Clicking the button, you will be prompted to choose an option to open a remote window. Choose \"Attach to a Running Container...\", and select \"Aperture_dev\" (which should be the only option). This will open a new window and the lower left corner should now display \"Container fizban007/aperture_dev (Aperture_dev)\": After connecting to the container, select \"Open Folder...\" and choose /home/developer/Aperture4 . This should open the project and display all the source files and folders on the left panel: These are all the source files of Aperture , and some associated data analysis scripts. We will now need to install some development plugins on the container. The most notable ones are C/C++ , CMake Tools , clang-format , Better TOML and Jupyter . This way you can enable code auto-completion on all files inside the container, as well as use the integrated CMake utilities. You can also install C/C++ Extension Pack which will install CMake Tools automatically. Building the Code in VS Code After installing the extensions, you will see that the bottom bar of VS Code is now modified. It will show \"CMake: [Release]: Ready\" and \"No active kit\": You can click on the \"No active kit\" button to choose a kit. Choose \"GCC 12.2.0\" as that is the default compiler in the Docker image. Then click on the \"CMake:\" button and choose \"Release\" again. It will prompt VS Code to run cmake again with the correct setup. You should see the output above eventually say \"Build files have been written to: /home/developer/Aperture4/build\". This means that all build instructions generated by CMake are now in the \"Aperture/build\" directory. You can proceed to build the code by clicking on \"Build\", next to the kit selection. In the future, if you have changed some files and would like to recompile the code base, you can click \"Build\" again. Building the Code from Command Line Alternatively, you can also build the code from command line, without the use of VS Code. Starting inside the container, assuming you just cloned the Aperture4 repository, you can build the code using the following commands: $ cd ~/Aperture4 $ mkdir build $ cd build $ cmake .. $ make -j8 The cmake command generates the necessary compiler instructions and a Makefile , the make runs the commands inside that Makefile and compiles the entire codebase. The -j8 option tells make to use 8 parallel processes, significantly speeding up the compilation. Running Basic Unit Tests After you have built the code base, you can run the simple unit test suite using the command make check in the build directory. This will (ideally) perform a series of very basic functionality tests to make sure there is no catastrophic problems in the code base. Since the unit tests only cover very basic things, this is not a guarantee that all components of the code will operate correctly. You can also do this within VS Code. Click on the \"[all]\" button next to \"Build\", which will allow you to choose the compilation target. We want to choose check , then clicking on \"Build\" will effectively allow us to run make check in the build directory as if we were doing it in the terminal. Running a Simple Simulation Example I prepared two very basic simulation examples in the code base, under problems/examples . All of the different setups in Aperture are stored in the problems directory, and the compiled binaries go into the bin directory of each problem. For example, after you have successfully compiled the code base, you should be able to see two files under problems/examples/bin/ : em_wave and test_particle . Run the test_particle simulation using the following command: $ cd ~/Aperture4/problems/examples/bin $ ./test_particles This will simulate a bunch of test particles moving in a uniform magnetic field. After the simulation is completed, you can see that there is a new directory called Data generated under examples/bin . This directory contains the outputs from the simulation. You should see fld.[00000-01000].h5 which contains the field data for timesteps 0 through 1000, as well as ptc.[00000-01000].h5 which contains the tracked particle data. grid.h5 contains information about the grid, and config.toml contains the simulation parameters. Looking at Simulation Data Our main way to analyze simulation data is through python in a Jupyter notebook. You can navigate to python/Examples - Test Particle.ipynb inside VS Code. It will open up a jupyter notebook interface containing the code blocks. We want to start a jupyter server inside the container and attach to that server in our VS Code window. To do that, open a terminal in VS Code using the short cut Ctrl + Shift + `, or click on the menu \"Terminal -> New Terminal\". In the new terminal, navigate to the python directory using cd python , then run jupyter server . It will start a jupyter instance that can run python code in the notebook. You should be able to see something like this: Copy the url highlighted in the red rectangle. Click on the \"Jupyter Server: Remote\" button near the lower right, highlighted by the red arrow above. In the menu that pops up, click \"Existing\". It will should automatically paste the link that you just copied in the prompt. If not, paste it manually. Press enter (twice, the second prompt you can leave blank), jupyter should now be connected to the instance inside the container. You can verify that this is correctly set up by looking at the top right corner of the VS Code window: If it shows \"Python 3 (ipykernel)\" like shown in the figure, then you are done. If not, click on it, and choose \"Python 3 (ipykernel)\" which should also say \"(Remote) Jupyter Kernel\". Now you can execute the notebook. It will plot the trajectory of two particles, which should trace out a fat circle in the plane. Feel free to play with the notebook and look at different aspects of the trajectory. Simulating Electron Two-Stream Instability Let us try to use the code to run a simple realization of the electron two-stream instability. Aperture contains a setup ready, located in problems/two_stream . After you have successfully compiled the code using either VS Code or from the command line , there should be an executable file two_stream_1d inside the directory problems/two_stream/bin . In general, Aperture is organized such that every problem directory under problems will have src (which contains the source files of that problem setup) and bin (which contains the compiled binary files) directories. We will be using the configuration file config_1d.toml which is bundled with the code by default and located under problems/two_stream/bin . Review the contents of the file. It defines key numerical parameters such as the physical size of the simulation box, as well as the number of grid cells in each dimension. In particular, ranks determines how many MPI ranks are assigned to each dimension. The total number of ranks should map to the number of physical CPU cores used for the simulation. Adjust the number of ranks in config_1d.toml to the number of CPU cores available in your Docker container. You can find this number by going to the Docker Desktop configuration: then click on \"Resources\". It will show how many cores on your machine are allocated to the container. For the purpose of this tutorial I will assume this number is 8. You can launch the two-stream simulation in the command line using: mpirun -np 8 ./two_stream_1d -c config_1d.toml We use mpirun because it sets up the MPI environment with the correct number of CPUs. -np 8 tells mpirun to use 8 physical cores, which you need to change to the number of ranks you assigned in the configuration file. ./two_stream_1d is the name of the executable, and the option -c config_1d.toml specifies that we will be using config_1d.toml as our configuration file. If we do not specify this option, Aperture defaults to use the file named config.toml . If no such a file is in the current directory, all configuration options will be taken as default (which may very likely not make sense for your application). Depending on the speed of your development machine, the simulation will take anywhere from 30 seconds to several minutes. After it is done, you can see a directory Data created in problems/two_stream/bin . This is the directory that contains simulation outputs and the one we will analyze. Analyzing Two-Stream Data Start a Jupyter server using the instructions above . Open the note book python/Two Stream Test.ipynb . You should be able to run the entire notebook. The first cell loads the necessary libraries. The second cell loads the two stream data that we produced in the previous step. The third cell plots a single snapshot of the phase space of electrons. The 4th cell creates such a plot for every output time steps. The plots are located under python/plots . You can look at the plots in VS Code as well. Finally the 5th cell computes the energy history of the electric field, and the 6th cell plots it. In order to make a movie with the plots, you can open a terminal in VS Code and run: cd python ffmpeg -y -f image2 -r 14 -i plots/%05d.png -c:v h264 -crf 18 -vf \"pad=ceil(iw/2)*2:ceil(ih/2)*2\" -pix_fmt yuv420p phase_space.mp4 If you do not have ffmpeg in the Docker container, remember to pull the latest image from Docker Hub and re-create your container using the instructions above . Once you successfully produced phase_space.mp4 , you can look at it in VS Code. You can also download it to your local machine by right clicking on the file and choose \"Download\".","title":"Two stream instability"},{"location":"4-two-stream/#tutorial-two-stream-instability","text":"In this tutorial, we will set up a Docker environment on your laptop for development of the code, and use it to run a basic 1D simulation of the electron two-stream instability.","title":"Tutorial: Two-stream Instability"},{"location":"4-two-stream/#install-docker-desktop","text":"You can download and install Docker Desktop here, regardless of your operating system: https://www.docker.com/products/docker-desktop/ Run \"Docker Desktop\". The first time it will try to setup your machine for Docker containers, so expect waiting for several minutes. But eventually you should see the main user interface of Docker Desktop. You can safely skip the initial tutorial. In order to prepare for our next steps, we want to create a persistent volume to hold all our development files. This can be done by clicking \"Volumes\" on the left panel, and then \"Create\" at the top right corner. Name it whatever you want, but I'll use vol_home as the volume name for the rest of this tutorial.","title":"Install Docker Desktop"},{"location":"4-two-stream/#pull-the-development-docker-image","text":"Click on the \"Search\" bar on the top right of Docker Desktop. Type in fizban007/aperture_dev . You will see my image shown below. You can download the image by clicking \"Pull\": Alternatively, you can open a terminal and run the following command docker pull fizban007/aperture_dev This will download the Docker image that contains all the development libraries necessary for compiling/debugging Aperture . The Docker image is pretty large at about 2.9GB, so the download will probably take a while depending on the speed of your internet connection. After the download is done, you should be able to see it under \"Images\" in the Docker Desktop. Now you can create a named container with this image and run it using the following command in a terminal (note: don't add extra spaces or the command may fail): $ docker run -it --mount source=vol_home,target=/home/developer --name Aperture_dev fizban007/aperture_dev Note that we referred to the volume we created in the previous step, and mounted it at /home/developer . The reason we mount the volume here is that the default user in the Docker image is developer , and this is the default home directory. The command above will create a new container with the name Aperture_dev and put you under a bash shell inside the Docker container. In the Docker Desktop GUI, you should be able to see our new container \"Aperture_dev\" in the \"Containers\" tab. You can checkout our PIC code Aperture using the following command inside the container: $ cd ~ $ git clone https://github.com/fizban007/Aperture4.git --branch develop This will create a directory called Aperture4 in your mounted volume (inside the home directory of user developer ). This will be the directory we work with in the future. Note we cloned the develop branch, which is the most up-to-date and actively maintained one. We only need to create the container for one time. Afterwards, if you want to restart the container, the docker run command above will fail because we already have a container named \"Aperture_dev\". Instead, you can do: $ docker start Aperture_dev $ docker attach This will start the container and put you under a command line prompt.","title":"Pull the development Docker image"},{"location":"4-two-stream/#install-visual-studio-code","text":"Download and install VS Code from here: https://code.visualstudio.com/ Run VS Code. You will be directed to a \"Get started\" page. Click on the 5th icon on the left panel marked \"Extensions\". We want to install the \"Dev Containers\" extension to allow us to attach to a running Docker container. After installing \"Dev Containers\" (and possibly reloading VS Code), you should be able see a green button at the lower left corner of the window: Clicking the button, you will be prompted to choose an option to open a remote window. Choose \"Attach to a Running Container...\", and select \"Aperture_dev\" (which should be the only option). This will open a new window and the lower left corner should now display \"Container fizban007/aperture_dev (Aperture_dev)\": After connecting to the container, select \"Open Folder...\" and choose /home/developer/Aperture4 . This should open the project and display all the source files and folders on the left panel: These are all the source files of Aperture , and some associated data analysis scripts. We will now need to install some development plugins on the container. The most notable ones are C/C++ , CMake Tools , clang-format , Better TOML and Jupyter . This way you can enable code auto-completion on all files inside the container, as well as use the integrated CMake utilities. You can also install C/C++ Extension Pack which will install CMake Tools automatically.","title":"Install Visual Studio Code"},{"location":"4-two-stream/#building-the-code-in-vs-code","text":"After installing the extensions, you will see that the bottom bar of VS Code is now modified. It will show \"CMake: [Release]: Ready\" and \"No active kit\": You can click on the \"No active kit\" button to choose a kit. Choose \"GCC 12.2.0\" as that is the default compiler in the Docker image. Then click on the \"CMake:\" button and choose \"Release\" again. It will prompt VS Code to run cmake again with the correct setup. You should see the output above eventually say \"Build files have been written to: /home/developer/Aperture4/build\". This means that all build instructions generated by CMake are now in the \"Aperture/build\" directory. You can proceed to build the code by clicking on \"Build\", next to the kit selection. In the future, if you have changed some files and would like to recompile the code base, you can click \"Build\" again.","title":"Building the Code in VS Code"},{"location":"4-two-stream/#building-the-code-from-command-line","text":"Alternatively, you can also build the code from command line, without the use of VS Code. Starting inside the container, assuming you just cloned the Aperture4 repository, you can build the code using the following commands: $ cd ~/Aperture4 $ mkdir build $ cd build $ cmake .. $ make -j8 The cmake command generates the necessary compiler instructions and a Makefile , the make runs the commands inside that Makefile and compiles the entire codebase. The -j8 option tells make to use 8 parallel processes, significantly speeding up the compilation.","title":"Building the Code from Command Line"},{"location":"4-two-stream/#running-basic-unit-tests","text":"After you have built the code base, you can run the simple unit test suite using the command make check in the build directory. This will (ideally) perform a series of very basic functionality tests to make sure there is no catastrophic problems in the code base. Since the unit tests only cover very basic things, this is not a guarantee that all components of the code will operate correctly. You can also do this within VS Code. Click on the \"[all]\" button next to \"Build\", which will allow you to choose the compilation target. We want to choose check , then clicking on \"Build\" will effectively allow us to run make check in the build directory as if we were doing it in the terminal.","title":"Running Basic Unit Tests"},{"location":"4-two-stream/#running-a-simple-simulation-example","text":"I prepared two very basic simulation examples in the code base, under problems/examples . All of the different setups in Aperture are stored in the problems directory, and the compiled binaries go into the bin directory of each problem. For example, after you have successfully compiled the code base, you should be able to see two files under problems/examples/bin/ : em_wave and test_particle . Run the test_particle simulation using the following command: $ cd ~/Aperture4/problems/examples/bin $ ./test_particles This will simulate a bunch of test particles moving in a uniform magnetic field. After the simulation is completed, you can see that there is a new directory called Data generated under examples/bin . This directory contains the outputs from the simulation. You should see fld.[00000-01000].h5 which contains the field data for timesteps 0 through 1000, as well as ptc.[00000-01000].h5 which contains the tracked particle data. grid.h5 contains information about the grid, and config.toml contains the simulation parameters.","title":"Running a Simple Simulation Example"},{"location":"4-two-stream/#looking-at-simulation-data","text":"Our main way to analyze simulation data is through python in a Jupyter notebook. You can navigate to python/Examples - Test Particle.ipynb inside VS Code. It will open up a jupyter notebook interface containing the code blocks. We want to start a jupyter server inside the container and attach to that server in our VS Code window. To do that, open a terminal in VS Code using the short cut Ctrl + Shift + `, or click on the menu \"Terminal -> New Terminal\". In the new terminal, navigate to the python directory using cd python , then run jupyter server . It will start a jupyter instance that can run python code in the notebook. You should be able to see something like this: Copy the url highlighted in the red rectangle. Click on the \"Jupyter Server: Remote\" button near the lower right, highlighted by the red arrow above. In the menu that pops up, click \"Existing\". It will should automatically paste the link that you just copied in the prompt. If not, paste it manually. Press enter (twice, the second prompt you can leave blank), jupyter should now be connected to the instance inside the container. You can verify that this is correctly set up by looking at the top right corner of the VS Code window: If it shows \"Python 3 (ipykernel)\" like shown in the figure, then you are done. If not, click on it, and choose \"Python 3 (ipykernel)\" which should also say \"(Remote) Jupyter Kernel\". Now you can execute the notebook. It will plot the trajectory of two particles, which should trace out a fat circle in the plane. Feel free to play with the notebook and look at different aspects of the trajectory.","title":"Looking at Simulation Data"},{"location":"4-two-stream/#simulating-electron-two-stream-instability","text":"Let us try to use the code to run a simple realization of the electron two-stream instability. Aperture contains a setup ready, located in problems/two_stream . After you have successfully compiled the code using either VS Code or from the command line , there should be an executable file two_stream_1d inside the directory problems/two_stream/bin . In general, Aperture is organized such that every problem directory under problems will have src (which contains the source files of that problem setup) and bin (which contains the compiled binary files) directories. We will be using the configuration file config_1d.toml which is bundled with the code by default and located under problems/two_stream/bin . Review the contents of the file. It defines key numerical parameters such as the physical size of the simulation box, as well as the number of grid cells in each dimension. In particular, ranks determines how many MPI ranks are assigned to each dimension. The total number of ranks should map to the number of physical CPU cores used for the simulation. Adjust the number of ranks in config_1d.toml to the number of CPU cores available in your Docker container. You can find this number by going to the Docker Desktop configuration: then click on \"Resources\". It will show how many cores on your machine are allocated to the container. For the purpose of this tutorial I will assume this number is 8. You can launch the two-stream simulation in the command line using: mpirun -np 8 ./two_stream_1d -c config_1d.toml We use mpirun because it sets up the MPI environment with the correct number of CPUs. -np 8 tells mpirun to use 8 physical cores, which you need to change to the number of ranks you assigned in the configuration file. ./two_stream_1d is the name of the executable, and the option -c config_1d.toml specifies that we will be using config_1d.toml as our configuration file. If we do not specify this option, Aperture defaults to use the file named config.toml . If no such a file is in the current directory, all configuration options will be taken as default (which may very likely not make sense for your application). Depending on the speed of your development machine, the simulation will take anywhere from 30 seconds to several minutes. After it is done, you can see a directory Data created in problems/two_stream/bin . This is the directory that contains simulation outputs and the one we will analyze.","title":"Simulating Electron Two-Stream Instability"},{"location":"4-two-stream/#analyzing-two-stream-data","text":"Start a Jupyter server using the instructions above . Open the note book python/Two Stream Test.ipynb . You should be able to run the entire notebook. The first cell loads the necessary libraries. The second cell loads the two stream data that we produced in the previous step. The third cell plots a single snapshot of the phase space of electrons. The 4th cell creates such a plot for every output time steps. The plots are located under python/plots . You can look at the plots in VS Code as well. Finally the 5th cell computes the energy history of the electric field, and the 6th cell plots it. In order to make a movie with the plots, you can open a terminal in VS Code and run: cd python ffmpeg -y -f image2 -r 14 -i plots/%05d.png -c:v h264 -crf 18 -vf \"pad=ceil(iw/2)*2:ceil(ih/2)*2\" -pix_fmt yuv420p phase_space.mp4 If you do not have ffmpeg in the Docker container, remember to pull the latest image from Docker Hub and re-create your container using the instructions above . Once you successfully produced phase_space.mp4 , you can look at it in VS Code. You can also download it to your local machine by right clicking on the file and choose \"Download\".","title":"Analyzing Two-Stream Data"},{"location":"about/","text":"About","title":"About"},{"location":"about/#about","text":"","title":"About"}]}